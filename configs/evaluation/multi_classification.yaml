prepare_cfg: null

# Which keys/attributes are supposed to be collected from `outputs` and `batch`
step_outputs:
  outputs:
    - "preds" # can be a str
    - "logits"
  batch: # or a list[str]
    - labels
    - g

# either metrics or val_metrics and test_metrics
# where the latter
# metrics_cfg should be copied for each dataset by default unless _datasets_ is specified
metrics_cfg:
  # name of the metric used eg for logging
  acc:
    # instructions to instantiate metric, preferrably torchmetrics.Metric
    metric:
      _partial_: true
      _target_: torchmetrics.functional.accuracy
      task: "multiclass"
      num_classes: 28
      average: "micro"
    # either "eval_step" or "epoch_end"
    compute_on: "epoch_end"
    kwargs: 
      preds: "outputs:preds"
      target: "outputs:labels"
  acc_macro:
    # instructions to instantiate metric, preferrably torchmetrics.Metric
    metric:
      _partial_: true
      _target_: torchmetrics.functional.accuracy
      task: "multiclass"
      num_classes: 28
      average: "macro"
    # either "eval_step" or "epoch_end"
    compute_on: "epoch_end"
    kwargs: 
      preds: "outputs:preds"
      target: "outputs:labels"
      
  # name of the AUC metric used for logging
  auc:
    metric:
      _partial_: true
      _target_: torchmetrics.functional.auroc
      task: "multiclass"
      num_classes: 28  # adjust the number of labels as needed
      average: "macro"
    compute_on: "epoch_end"
    kwargs: 
      preds: "outputs:preds"
      target: "outputs:labels"
      
  tpr_gap:
    # instructions to instantiate metric, preferrably torchmetrics.Metric
    metric:
      #_partial_: true
      _target_: src.evaluation.equalized_odds.TPRGap
    # either "eval_step" or "epoch_end"
    compute_on: "epoch_end"
    kwargs: 
      preds: "outputs:preds"
      target: "outputs:labels"
      gender: "outputs:g"